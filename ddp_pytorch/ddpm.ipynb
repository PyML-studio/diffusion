{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LinearNoiseScheduler():\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.step = 0\n",
    "\n",
    "        # pre-compute alphas and betas\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        # \\bar{\\alpha}_t}\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, 0)\n",
    "        # \\sqrt{\\bar{\\alpha}_t}}\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        # \\sqrt{1-\\bar{\\alpha}_t}}\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
    "\n",
    "    # forward process\n",
    "    def add_noise(self, original, noise, t):\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "\n",
    "        sqrt_alph_cum_prod = self.sqrt_alpha_cum_prod[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n",
    "\n",
    "        for _ in range(original.dim() - 1):\n",
    "            sqrt_alph_cum_prod = sqrt_alph_cum_prod.unsqueeze(-1)\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "\n",
    "        # \\sqrt{\\bar{\\alpha}_t}} * x_0 + (1-\\sqrt{\\bar{\\alpha}_t}) * \\epsilon_t\n",
    "        return sqrt_alph_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise\n",
    "    \n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        # x0 = (xt - \\sqrt{1-\\bar{\\alpha}_t}} * \\epsilon_t) / \\sqrt{\\bar{\\alpha}_t}}\n",
    "        x0 = (\n",
    "            xt - self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred\n",
    "        ) / self.sqrt_alpha_cum_prod[t]\n",
    "\n",
    "        x0 = torch.clamp(x0, -1, 1)\n",
    "\n",
    "        mean = xt - (self.betas[t] * noise_pred) / self.sqrt_one_minus_alpha_cum_prod[t]\n",
    "        mean = mean / torch.sqrt(self.alphas[t])\n",
    "\n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "\n",
    "        variance = (1 - self.alpha_cum_prod[t-1]) / (1 - self.alpha_cum_prod[t])\n",
    "        variance *= self.betas[t]\n",
    "        sigma = torch.sqrt(variance)\n",
    "        # sample from Gaussian distribution\n",
    "        z = torch.randn(xt.shape).to(xt.device)\n",
    "        return mean + sigma * z, x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet model\n",
    "\n",
    " * Using sinusoidal position embedding for time-embeddings\n",
    "\n",
    "$$sin\\left(pos / 10000^{2i / d_{model}}\\right)$$\n",
    "$$cos\\left(pos / 10000^{2i+1 / d_{model}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_time_embedding(time_steps, t_emb_dim):\n",
    "    device = time_steps.device\n",
    "    factor = 10000 ** ((\n",
    "        torch.arange(0, t_emb_dim//2, device=device) / (t_emb_dim // 2)\n",
    "    ))\n",
    "\n",
    "    t_emb = time_steps.unsqueeze(-1).repeat(1, t_emb_dim//2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "\n",
    "    return t_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            t_emb_dim,\n",
    "            down_sample,\n",
    "            num_heads\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_sample = down_sample\n",
    "        self.resnet_conv_first = nn.Sequential(\n",
    "            nn.GroupNorm(8, in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.t_emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim, out_channels),\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.Sequential(\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.attn_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            out_channels, num_heads, batch_first=True\n",
    "        )\n",
    "        self.resid_input_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.down_sample_conv = nn.Conv2d(\n",
    "            out_channels, out_channels, 4, 2, 1\n",
    "        ) if self.down_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        # ResNet block 1\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first(out)\n",
    "        # time embedding\n",
    "        out += self.t_emb_layers(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        # ResNet block 2\n",
    "        out = self.resnet_conv_second(out)\n",
    "        # residual\n",
    "        out += self.resid_input_conv(resnet_input)\n",
    "\n",
    "        # self-attention block\n",
    "        B, C, H, W = out.shape\n",
    "        input_for_attn = out.view(B, C, -1)\n",
    "        input_for_attn = self.attn_norm(input_for_attn)\n",
    "        input_for_attn = input_for_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attn(input_for_attn, input_for_attn, input_for_attn)\n",
    "        out_attn = out_attn.transpose(1, 2).view(B, C, H, W)\n",
    "        out += out_attn\n",
    "\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid-Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            t_emb_dim,\n",
    "            num_heads\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet_conv_first = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, in_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.attn_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            out_channels, num_heads, batch_first=True\n",
    "        )\n",
    "        self.resid_input_conv = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.Conv2d(out_channels, out_channels, 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        # First ResNet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out += self.t_emb_layers[0](t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out += self.resid_input_conv[0](resnet_input)\n",
    "\n",
    "        # self-attention block\n",
    "        B, C, H, W = out.shape\n",
    "        input_for_attn = out.view(B, C, -1)\n",
    "        input_for_attn = self.attn_norm(input_for_attn)\n",
    "        input_for_attn = input_for_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attn(input_for_attn, input_for_attn, input_for_attn)\n",
    "        out_attn = out_attn.transpose(1, 2).view(B, C, H, W)\n",
    "        out += out_attn\n",
    "\n",
    "        # Second ResNet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[1](out)\n",
    "        out += self.t_emb_layers[1](t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        out = self.resnet_conv_second[1](out)\n",
    "        out += self.resid_input_conv[1](resnet_input)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample-Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            t_emb_dim,\n",
    "            up_sample,\n",
    "            num_heads\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up_sample = up_sample\n",
    "        self.resnet_conv_first = nn.Sequential(\n",
    "            nn.GroupNorm(8, in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.t_emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim, out_channels),\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.Sequential(\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.attn_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            out_channels, num_heads, batch_first=True\n",
    "        )\n",
    "        self.resid_input_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(\n",
    "            in_channels // 2, in_channels // 2, 4, 2, 1\n",
    "        ) if self.up_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "\n",
    "        # ResNet block\n",
    "        out = x\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first(out)\n",
    "        out += self.t_emb_layers(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        out = self.resnet_conv_second(out)\n",
    "        out += self.resid_input_conv(resnet_input)\n",
    "\n",
    "        # self-attention block\n",
    "        B, C, H, W = out.shape\n",
    "        input_for_attn = out.view(B, C, -1)\n",
    "        input_for_attn = self.attn_norm(input_for_attn)\n",
    "        input_for_attn = input_for_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attn(input_for_attn, input_for_attn, input_for_attn)\n",
    "        out_attn = out_attn.transpose(1, 2).view(B, C, H, W)\n",
    "        out += out_attn\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together in UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_channels = [32, 64, 128, 256]\n",
    "        self.mid_channels = [256, 256, 128]\n",
    "        self.t_emb_dim = 128\n",
    "\n",
    "        self.down_samples = [True, True, False]\n",
    "        self.up_samples = list(reversed(self.down_samples))\n",
    "\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "        )\n",
    "        self.conv_inp_layer = nn.Conv2d(img_channels, self.down_channels[0], 3, 1, 1)\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(\n",
    "                DownBlock(\n",
    "                    self.down_channels[i],\n",
    "                    self.down_channels[i+1],\n",
    "                    self.t_emb_dim,\n",
    "                    down_sample=self.down_samples[i],\n",
    "                    num_heads=4\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels)-1):\n",
    "            self.mids.append(\n",
    "                MidBlock(\n",
    "                    self.down_channels[i],\n",
    "                    self.mid_channels[i+1],\n",
    "                    self.t_emb_dim,\n",
    "                    num_heads=4\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels)-1):\n",
    "            self.ups.append(\n",
    "                UpBlock(\n",
    "                    self.down_channels[i] * 2,\n",
    "                    self.down_channels[i-1] if i != 0 else 16,\n",
    "                    self.t_emb_dim,\n",
    "                    up_sample=self.up_samples[i],\n",
    "                    num_heads=4\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(8, 16)\n",
    "        self.conv_out = nn.Conv2d(16, img_channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        print(\"Time embedding:\", t_emb.shape)\n",
    "        print(\"Input:\", x.shape)\n",
    "\n",
    "        out = self.conv_inp_layer(x)\n",
    "        out_downs = []\n",
    "        for down_layer in self.downs:\n",
    "            print(\"Down:\", out.shape)\n",
    "            out_downs.append(out)\n",
    "            out = down_layer(out, t_emb)\n",
    "\n",
    "        for mid_layer in self.mids:\n",
    "            print(\"Mid:\", out.shape)\n",
    "            print(\"mid_layer:\", mid_layer)\n",
    "            out = mid_layer(out, t_emb)\n",
    "\n",
    "        for up_layer in self.ups:\n",
    "            print(\"Up:\", out.shape)\n",
    "            out = up_layer(out, out_downs.pop(), t_emb)\n",
    "\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor(-1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# scale the pixels to the range [-1, 1]\n",
    "def scale(x):\n",
    "    return x * 2 - 1\n",
    "\n",
    "def unscale(x):\n",
    "    return (x + 1) / 2\n",
    "\n",
    "mnist = MNIST(\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.dataset[idx]\n",
    "        img = scale(img)\n",
    "        return img\n",
    "    \n",
    "# testing:\n",
    "dataset = MNISTDataset(mnist)\n",
    "print(next(iter(dataset)).shape)\n",
    "print(next(iter(dataset))[0].min(), next(iter(dataset))[0].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the config file:\n",
    "import yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the noise scheduler:\n",
    "scheduler = LinearNoiseScheduler(\n",
    "    config[\"diffusion\"][\"num_timesteps\"],\n",
    "    config[\"diffusion\"][\"beta_start\"],\n",
    "    config[\"diffusion\"][\"beta_end\"]\n",
    ")\n",
    "\n",
    "# create the model:\n",
    "model = UNet(config[\"model\"][\"img_channels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = config[\"train\"][\"num_epochs\"]\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config[\"train\"][\"lr\"]\n",
    ")\n",
    "critereon = nn.MSELoss()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config[\"train\"][\"batch_size\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Time embedding: torch.Size([64, 128])\n",
      "Input: torch.Size([64, 1, 28, 28])\n",
      "Down: torch.Size([64, 32, 28, 28])\n",
      "Down: torch.Size([64, 64, 14, 14])\n",
      "Down: torch.Size([64, 128, 7, 7])\n",
      "Mid: torch.Size([64, 256, 7, 7])\n",
      "mid_layer: MidBlock(\n",
      "  (resnet_conv_first): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(32, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (t_emb_layers): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): SiLU()\n",
      "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (resnet_conv_second): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (attn_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (resid_input_conv): ModuleList(\n",
      "    (0): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [32] and input of shape [64, 256, 7, 7]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(noisy_imgs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# predict noise:\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# compute loss and backpropagate\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m critereon(noise_pred, noise)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMid:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmid_layer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mid_layer)\n\u001b[0;32m---> 73\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmid_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m up_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mups:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUp:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mMidBlock.forward\u001b[0;34m(self, x, t_emb)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# First ResNet block\u001b[39;00m\n\u001b[1;32m     60\u001b[0m resnet_input \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m---> 61\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet_conv_first\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_emb_layers[\u001b[38;5;241m0\u001b[39m](t_emb)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet_conv_second[\u001b[38;5;241m0\u001b[39m](out)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/normalization.py:288\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/functional.py:2606\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2605\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [32] and input of shape [64, 256, 7, 7]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for imgs in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        print(imgs.shape)\n",
    "        # sample noise\n",
    "        noise = torch.randn_like(imgs).to(device)\n",
    "        # sample timestep\n",
    "        t = torch.randint(\n",
    "            0, config[\"diffusion\"][\"num_timesteps\"],\n",
    "            (imgs.shape[0],)\n",
    "        ).to(device)\n",
    "\n",
    "        # add noise to imgs: add_noise(original, noise, t)\n",
    "        noisy_imgs = scheduler.add_noise(imgs, noise, t)\n",
    "        print(noisy_imgs.shape)\n",
    "\n",
    "        # predict noise:\n",
    "        noise_pred = model(noisy_imgs, t)\n",
    "\n",
    "        # compute loss and backpropagate\n",
    "        loss = critereon(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        break\n",
    "    print(f\"Epoch {epoch} Loss: {sum(losses) / len(losses)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
