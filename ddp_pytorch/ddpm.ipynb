{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LinearNoiseScheduler():\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.step = 0\n",
    "\n",
    "        # pre-compute alphas and betas\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        # \\bar{\\alpha}_t}\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, 0)\n",
    "        # \\sqrt{\\bar{\\alpha}_t}}\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        # \\sqrt{1-\\bar{\\alpha}_t}}\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
    "\n",
    "    # forward process\n",
    "    def add_noise(self, original, noise, t):\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "\n",
    "        sqrt_alph_cum_prod = self.sqrt_alpha_cum_prod[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n",
    "\n",
    "        for _ in range(original.dim() - 1):\n",
    "            sqrt_alph_cum_prod = sqrt_alph_cum_prod.unsqueeze(-1)\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "\n",
    "        # \\sqrt{\\bar{\\alpha}_t}} * x_0 + (1-\\sqrt{\\bar{\\alpha}_t}) * \\epsilon_t\n",
    "        return sqrt_alph_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise\n",
    "    \n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        # x0 = (xt - \\sqrt{1-\\bar{\\alpha}_t}} * \\epsilon_t) / \\sqrt{\\bar{\\alpha}_t}}\n",
    "        x0 = (\n",
    "            xt - self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred\n",
    "        ) / self.sqrt_alpha_cum_prod[t]\n",
    "\n",
    "        x0 = torch.clamp(x0, -1, 1)\n",
    "\n",
    "        mean = xt - (self.betas[t] * noise_pred) / self.sqrt_one_minus_alpha_cum_prod[t]\n",
    "        mean = mean / torch.sqrt(self.alphas[t])\n",
    "\n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "\n",
    "        variance = (1 - self.alpha_cum_prod[t-1]) / (1 - self.alpha_cum_prod[t])\n",
    "        variance *= self.betas[t]\n",
    "        sigma = torch.sqrt(variance)\n",
    "        # sample from Gaussian distribution\n",
    "        z = torch.randn(xt.shape).to(xt.device)\n",
    "        return mean + sigma * z, x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet model\n",
    "\n",
    " * Using sinusoidal position embedding for time-embeddings\n",
    "\n",
    "$$sin\\left(pos / 10000^{2i / d_{model}}\\right)$$\n",
    "$$cos\\left(pos / 10000^{2i+1 / d_{model}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_time_embedding(time_steps, t_emb_dim):\n",
    "    device = time_steps.device\n",
    "    factor = 10000 ** ((\n",
    "        torch.arange(0, t_emb_dim//2, device=device) / (t_emb_dim // 2)\n",
    "    ))\n",
    "\n",
    "    t_emb = time_steps.unsqueeze(-1).repeat(1, t_emb_dim//2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "\n",
    "    return t_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            t_emb_dim,\n",
    "            down_sample,\n",
    "            num_heads\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_sample = down_sample\n",
    "        self.resnet_conv_first = nn.Sequential(\n",
    "            nn.GroupNorm(8, in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.t_emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim, out_channels),\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.Sequential(\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.attn_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            out_channels, num_heads, batch_first=True\n",
    "        )\n",
    "        self.resid_input_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.down_sample_conv = nn.Conv2d(\n",
    "            out_channels, out_channels, 4, 2, 1\n",
    "        ) if self.down_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        # ResNet block 1\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first(out)\n",
    "        # time embedding\n",
    "        out += self.t_emb_layers(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        # ResNet block 2\n",
    "        out = self.resnet_conv_second(out)\n",
    "        # residual\n",
    "        out = out + self.resid_input_conv(resnet_input)\n",
    "\n",
    "        # self-attention block\n",
    "        B, C, H, W = out.shape\n",
    "        input_for_attn = out.view(B, C, -1)\n",
    "        input_for_attn = self.attn_norm(input_for_attn)\n",
    "        input_for_attn = input_for_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attn(input_for_attn, input_for_attn, input_for_attn)\n",
    "        out_attn = out_attn.transpose(1, 2).view(B, C, H, W)\n",
    "        out = out + out_attn\n",
    "\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mid-Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            t_emb_dim,\n",
    "            num_heads\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet_conv_first = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, in_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.attn_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            out_channels, num_heads, batch_first=True\n",
    "        )\n",
    "        self.resid_input_conv = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.Conv2d(out_channels, out_channels, 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        # First ResNet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layers[0](t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.resid_input_conv[0](resnet_input)\n",
    "\n",
    "        # self-attention block\n",
    "        B, C, H, W = out.shape\n",
    "        input_for_attn = out.view(B, C, -1)\n",
    "        input_for_attn = self.attn_norm(input_for_attn)\n",
    "        input_for_attn = input_for_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attn(input_for_attn, input_for_attn, input_for_attn)\n",
    "        out_attn = out_attn.transpose(1, 2).view(B, C, H, W)\n",
    "        out = out + out_attn\n",
    "\n",
    "        # Second ResNet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[1](out)\n",
    "        out = out + self.t_emb_layers[1](t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        out = self.resnet_conv_second[1](out)\n",
    "        out = out + self.resid_input_conv[1](resnet_input)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample-Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            t_emb_dim,\n",
    "            up_sample,\n",
    "            num_heads\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up_sample = up_sample\n",
    "        self.resnet_conv_first = nn.Sequential(\n",
    "            nn.GroupNorm(8, in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.t_emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim, out_channels),\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.Sequential(\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.attn_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            out_channels, num_heads, batch_first=True\n",
    "        )\n",
    "        self.resid_input_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(\n",
    "            in_channels // 2, in_channels // 2, 4, 2, 1\n",
    "        ) if self.up_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        print(x.shape, out_down.shape)\n",
    "        x = self.up_sample_conv(x)\n",
    "        print(\"after up_sample_conv\", x.shape)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "\n",
    "        # ResNet block\n",
    "        out = x\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first(out)\n",
    "        out = out + self.t_emb_layers(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        out = self.resnet_conv_second(out)\n",
    "        out = out + self.resid_input_conv(resnet_input)\n",
    "\n",
    "        # self-attention block\n",
    "        B, C, H, W = out.shape\n",
    "        input_for_attn = out.view(B, C, -1)\n",
    "        input_for_attn = self.attn_norm(input_for_attn)\n",
    "        input_for_attn = input_for_attn.transpose(1, 2)\n",
    "        out_attn, _ = self.attn(input_for_attn, input_for_attn, input_for_attn)\n",
    "        out_attn = out_attn.transpose(1, 2).view(B, C, H, W)\n",
    "        out = out + out_attn\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together in UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down_channels = [32, 64, 128, 256]\n",
    "        self.mid_channels = [256, 256, 128]\n",
    "        self.t_emb_dim = 128\n",
    "\n",
    "        self.down_samples = [True, True, False]\n",
    "\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "        )\n",
    "        self.conv_inp_layer = nn.Conv2d(img_channels, self.down_channels[0], 3, 1, 1)\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(\n",
    "                DownBlock(\n",
    "                    self.down_channels[i],\n",
    "                    self.down_channels[i+1],\n",
    "                    self.t_emb_dim,\n",
    "                    down_sample=self.down_samples[i],\n",
    "                    num_heads=4\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels)-1):\n",
    "            self.mids.append(\n",
    "                MidBlock(\n",
    "                    self.mid_channels[i],\n",
    "                    self.mid_channels[i+1],\n",
    "                    self.t_emb_dim,\n",
    "                    num_heads=4\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels)-1)):\n",
    "            self.ups.append(\n",
    "                UpBlock(\n",
    "                    self.down_channels[i] * 2,\n",
    "                    self.down_channels[i-1] if i != 0 else 16,\n",
    "                    self.t_emb_dim,\n",
    "                    up_sample=self.down_samples[i],\n",
    "                    num_heads=4\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(8, 16)\n",
    "        self.conv_out = nn.Conv2d(16, img_channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        print(\"Time embedding:\", t_emb.shape)\n",
    "        print(\"Input:\", x.shape)\n",
    "\n",
    "        out = self.conv_inp_layer(x)\n",
    "        out_downs = []\n",
    "        for down_layer in self.downs:\n",
    "            print(\"Down:\", out.shape)\n",
    "            out_downs.append(out)\n",
    "            out = down_layer(out, t_emb)\n",
    "\n",
    "        for mid_layer in self.mids:\n",
    "            print(\"Mid:\", out.shape)\n",
    "            #print(\"mid_layer:\", mid_layer)\n",
    "            out = mid_layer(out, t_emb)\n",
    "\n",
    "        for up_layer in self.ups:\n",
    "            out_down = out_downs.pop()\n",
    "            print(\"Up:\", out.shape, out_down.shape)\n",
    "            print(\"up_layer:\", up_layer.up_sample)\n",
    "            out = up_layer(out, out_down, t_emb)\n",
    "\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor(-1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# scale the pixels to the range [-1, 1]\n",
    "def scale(x):\n",
    "    return x * 2 - 1\n",
    "\n",
    "def unscale(x):\n",
    "    return (x + 1) / 2\n",
    "\n",
    "mnist = MNIST(\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.dataset[idx]\n",
    "        img = scale(img)\n",
    "        return img\n",
    "    \n",
    "# testing:\n",
    "dataset = MNISTDataset(mnist)\n",
    "print(next(iter(dataset)).shape)\n",
    "print(next(iter(dataset))[0].min(), next(iter(dataset))[0].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the config file:\n",
    "import yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the noise scheduler:\n",
    "scheduler = LinearNoiseScheduler(\n",
    "    config[\"diffusion\"][\"num_timesteps\"],\n",
    "    config[\"diffusion\"][\"beta_start\"],\n",
    "    config[\"diffusion\"][\"beta_end\"]\n",
    ")\n",
    "\n",
    "# create the model:\n",
    "model = UNet(config[\"model\"][\"img_channels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = config[\"train\"][\"num_epochs\"]\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config[\"train\"][\"lr\"]\n",
    ")\n",
    "critereon = nn.MSELoss()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config[\"train\"][\"batch_size\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "Time embedding: torch.Size([64, 128])\n",
      "Input: torch.Size([64, 1, 28, 28])\n",
      "Down: torch.Size([64, 32, 28, 28])\n",
      "Down: torch.Size([64, 64, 14, 14])\n",
      "Down: torch.Size([64, 128, 7, 7])\n",
      "Mid: torch.Size([64, 256, 7, 7])\n",
      "Mid: torch.Size([64, 256, 7, 7])\n",
      "Up: torch.Size([64, 128, 7, 7]) torch.Size([64, 128, 7, 7])\n",
      "up_layer: False\n",
      "torch.Size([64, 128, 7, 7]) torch.Size([64, 128, 7, 7])\n",
      "after up_sample_conv torch.Size([64, 128, 7, 7])\n",
      "Up: torch.Size([64, 64, 7, 7]) torch.Size([64, 64, 14, 14])\n",
      "up_layer: True\n",
      "torch.Size([64, 64, 7, 7]) torch.Size([64, 64, 14, 14])\n",
      "after up_sample_conv torch.Size([64, 64, 14, 14])\n",
      "Up: torch.Size([64, 32, 14, 14]) torch.Size([64, 32, 28, 28])\n",
      "up_layer: True\n",
      "torch.Size([64, 32, 14, 14]) torch.Size([64, 32, 28, 28])\n",
      "after up_sample_conv torch.Size([64, 32, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for imgs in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        print(imgs.shape)\n",
    "        # sample noise\n",
    "        noise = torch.randn_like(imgs).to(device)\n",
    "        # sample timestep\n",
    "        t = torch.randint(\n",
    "            0, config[\"diffusion\"][\"num_timesteps\"],\n",
    "            (imgs.shape[0],)\n",
    "        ).to(device)\n",
    "\n",
    "        # add noise to imgs: add_noise(original, noise, t)\n",
    "        noisy_imgs = scheduler.add_noise(imgs, noise, t)\n",
    "        print(noisy_imgs.shape)\n",
    "\n",
    "        # predict noise:\n",
    "        noise_pred = model(noisy_imgs, t)\n",
    "\n",
    "        # compute loss and backpropagate\n",
    "        loss = critereon(noise_pred, noise)\n",
    "\n",
    "        loss.backward()\n",
    "        break\n",
    "    break\n",
    "\n",
    "# save the model\n",
    "#torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args):\n",
    "    # read configs\n",
    "    with open(args.config, \"r\") as f:\n",
    "        try:\n",
    "            config = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            return None\n",
    "        \n",
    "    diffusion_config = config[\"diffusion_params\"]\n",
    "    model_config = config[\"model_params\"]\n",
    "    train_config = config[\"train_params\"]\n",
    "\n",
    "    # loade model\n",
    "    model = UNet(model_config[\"img_channels\"])\n",
    "    model.to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(args.model_path),\n",
    "        map_location=device\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # create the noise scheduler:\n",
    "    scheduler = LinearNoiseScheduler(\n",
    "        diffusion_config[\"num_timesteps\"],\n",
    "        diffusion_config[\"beta_start\"],\n",
    "        diffusion_config[\"beta_end\"]\n",
    "    )\n",
    "\n",
    "    # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
